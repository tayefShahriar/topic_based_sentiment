# -*- coding: utf-8 -*-
"""test_final_topic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DMhSe7gOIf9EfK_jRnVHh7rOJmPh-Wnz
"""

import numpy as np
import pandas as pd
import os
import nltk
import spacy
from wordcloud import WordCloud
import tensorflow as tf
from tensorflow.keras.preprocessing import text
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.utils import to_categorical
from nltk.corpus import stopwords, words
from nltk.stem import WordNetLemmatizer
import gensim #the library for Topic modelling
from gensim.models.ldamulticore import LdaMulticore
from gensim import corpora, models
from itertools import chain
import time
import re
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.metrics import AUC
from sklearn.metrics import confusion_matrix, classification_report
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
stop = set(stopwords.words('english'))

train_data = pd.read_csv('Corona_NLP_train.csv', encoding='latin-1')
test_data = pd.read_csv('Corona_NLP_test.csv', encoding='latin-1')

REPLACE_BY_SPACE = re.compile('[/(){}\[\]\|,;&-_]') 
def preprocess_text(text):
    text = text.lower()                                    
    text = re.sub(r"http\S+", "", text)                    
    text = re.sub(r"\@\S+", "", text)                      
    text = re.sub(r"#\S+", "", text)                       
    text = re.sub(r"won\'t", "would not", text)            
    text = re.sub(r"n\'t", " not", text)                   
    text = REPLACE_BY_SPACE.sub(' ', text)            
    text = [word.strip() for word in text.split()]         
    text = [re.sub(r'[^\u0020-\u007F]+', '', sentence) for sentence in text]
    text = [word for word in text if word not in stop]
    text = [word for word in text if len(word)>2]          
    text = [word for word in text if word!='amp']  
    #text = [word.split() for word in text]        
    text = ' '.join(text)
    return text

train_data['Tweet'] = train_data['OriginalTweet'].apply(preprocess_text)
test_data['Tweet'] = test_data['OriginalTweet'].apply(preprocess_text)

lemma = WordNetLemmatizer()

def clean(text):
    normalized = ' '.join([lemma.lemmatize(word) for word in text.split()])
    return normalized.split()

text_clean=train_data['Tweet'].apply(clean)

text_clean

train_data['text_clean'] = text_clean

#create dictionary
dictionary = corpora.Dictionary(text_clean)

corpus = [dictionary.doc2bow(text) for text in text_clean]

import spacy
nlp = spacy.load('en')

termsA = []
termsS = []
for review in nlp.pipe(train_data.Tweet):
    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']
    termsA.append(' '.join(chunks))
    if review.is_parsed:
        termsS.append(' '.join([token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == "ADJ" or token.pos_ == "VERB"))]))
    else:
        termsS.append('')  
train_data['termsA'] = termsA
train_data['termsS'] = termsS
train_data.head(10)

!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip
!unzip mallet-2.0.8.zip

mallet_path = 'mallet-2.0.8/bin/mallet'
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=dictionary)

# Show Topics
from pprint import pprint
pprint(ldamallet.show_topics(formatted=False))

import gensim    
model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)

# Compute Coherence Score
from gensim.models import CoherenceModel
coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=text_clean, dictionary=dictionary, coherence='c_v')
coherence_ldamallet = coherence_model_ldamallet.get_coherence()
print('\nCoherence Score: ', coherence_ldamallet)

def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    """
    Compute c_v coherence for various number of topics

    Parameters:
    ----------
    dictionary : Gensim dictionary
    corpus : Gensim corpus
    texts : List of input texts
    limit : Max num of topics

    Returns:
    -------
    model_list : List of LDA topic models
    coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    """
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values

# Can take a long time to run.
model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=text_clean, start=2, limit=40, step=6)

# Show graph
limit=40; start=2; step=6;
x = range(start, limit, step)
plt.plot(x, coherence_values)
plt.xlabel("Num Topics")
plt.ylabel("Coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show()

# Print the coherence scores
for m, cv in zip(x, coherence_values):
    print("Num Topics =", m, " has Coherence Value of", round(cv, 4))

# Select the model and print the topics
optimal_model = model_list[2]
model_topics = optimal_model.show_topics(formatted=False)
pprint(optimal_model.print_topics(num_words=20))

def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=text_clean):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)


df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=text_clean)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']

# Show
df_dominant_topic.head(10)

df_dominant_topic['aspect_terms'] = train_data['termsA']
df_dominant_topic['sentiment_terms'] = train_data['termsS']
df_dominant_topic.tail()

topic0A = []
topic1A = []
topic2A = []
topic3A = []
topic4A = []
topic5A = []
topic6A = []
topic7A = []
topic8A = []
topic9A = []
topic10A = []
topic11A = []
topic12A = []
topic13A = []
topic0S = []
topic1S = []
topic2S = []
topic3S = []
topic4S = []
topic5S = []
topic6S = []
topic7S = []
topic8S = []
topic9S = []
topic10S = []
topic11S = []
topic12S = []
topic13S = []

for i in range(len(df_dominant_topic.Dominant_Topic)):
  #occupied_space.append(space[i:i+x])
  if df_dominant_topic.Dominant_Topic[i] == 0.0:
    topic0A.append(df_dominant_topic.aspect_terms[i])
    topic0S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 1.0:
    topic1A.append(df_dominant_topic.aspect_terms[i])
    topic1S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 2.0:
    topic2A.append(df_dominant_topic.aspect_terms[i])    
    topic2S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 3.0:
    topic3A.append(df_dominant_topic.aspect_terms[i])
    topic3S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 4.0:
    topic4A.append(df_dominant_topic.aspect_terms[i])
    topic4S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 5.0:
    topic5A.append(df_dominant_topic.aspect_terms[i])
    topic5S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 6.0:
    topic6A.append(df_dominant_topic.aspect_terms[i])
    topic6S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 7.0:
    topic7A.append(df_dominant_topic.aspect_terms[i])
    topic7S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 8.0:
    topic8A.append(df_dominant_topic.aspect_terms[i])
    topic8S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 9.0:
    topic9A.append(df_dominant_topic.aspect_terms[i])    
    topic9S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 10.0:
    topic10A.append(df_dominant_topic.aspect_terms[i])
    topic10S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 11.0:
    topic11A.append(df_dominant_topic.aspect_terms[i])
    topic11S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 12.0:
    topic12A.append(df_dominant_topic.aspect_terms[i])
    topic12S.append(df_dominant_topic.sentiment_terms[i])
  if df_dominant_topic.Dominant_Topic[i] == 13.0:
    topic13A.append(df_dominant_topic.aspect_terms[i])
    topic13S.append(df_dominant_topic.sentiment_terms[i])

import re,string,unicodedata
def basic_clean(text):
  """
  A simple function to clean up the data. All the words that
  are not designated as a stop word is then lemmatized after
  encoding and basic regex parsing are performed.
  """
  wnl = nltk.stem.WordNetLemmatizer()
  stopwords = nltk.corpus.stopwords.words('english')
  text = (unicodedata.normalize('NFKD', text)
    .encode('ascii', 'ignore')
    .decode('utf-8', 'ignore')
    .lower())
  words = re.sub(r'[^\w\s]', '', text).split()
  return [wnl.lemmatize(word) for word in words if word not in stopwords]

nltk.download('wordnet')

HQ_words = basic_clean(''.join(str(topic13S)))
trigram_HQ = (pd.Series(nltk.ngrams(HQ_words, 1)).value_counts())[:20]
trigram_HQ = pd.DataFrame(trigram_HQ)
trigram_HQ['idx'] = trigram_HQ.index
trigram_HQ['idx'] = trigram_HQ.apply(lambda x: '('+x['idx'][0]+')',axis=1)

HQ_wordsA = basic_clean(''.join(str(topic13A)))
trigram_HQA = (pd.Series(nltk.ngrams(HQ_wordsA, 1)).value_counts())[:20]
trigram_HQA = pd.DataFrame(trigram_HQA)
trigram_HQA['idx'] = trigram_HQA.index
trigram_HQA['idx'] = trigram_HQA.apply(lambda x: '('+x['idx'][0]+')',axis=1)

import plotly.graph_objs as go
import plotly.offline as pyoff
plot_data = [
    go.Bar(
        x=trigram_HQ['idx'],
        y=trigram_HQ[0],
        marker = dict(
            color = 'blue'
        )
    )
]
plot_layout = go.Layout(
        title='Top 20 Unigrams from Covid-19 Tweets',
        yaxis_title='Count',
        xaxis_title='Unigram',
        font_size=20,
        plot_bgcolor='rgba(0,0,0,0)'
        
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

plot_data = [
    go.Bar(
        x=trigram_HQA['idx'],
        y=trigram_HQA[0],
        marker = dict(
            color = 'green'
        )
    )
]
plot_layout = go.Layout(
        title='Top 20 Unigrams from Covid-19 Tweets',
        yaxis_title='Count',
        xaxis_title='Unigram',
        font_size=20,
        plot_bgcolor='rgba(0,0,0,0)'
        
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

ntopic0 = []
ntopic1 = []
ntopic2 = []
ntopic3 = []
ntopic4 = []
ntopic5 = []
ntopic6 = []
ntopic7 = []
ntopic8 = []
ntopic9 = []
ntopic10 = []
ntopic11 = []
ntopic12 = []
ntopic13 = []

df_dominant_topic['Tweet'] = train_data['Tweet']

df_dominant_topic['OriginalTweet'] = train_data['OriginalTweet']

df_dominant_topic['Sentiment'] = train_data['Sentiment']

df_dominant_topic.to_csv(r'dominantOriginal2.csv', index = False)

for i in range(len(df_dominant_topic.Dominant_Topic)):
  #occupied_space.append(space[i:i+x])
  if df_dominant_topic.Dominant_Topic[i] == 0.0:
    ntopic0.append(df_dominant_topic.Tweet[i])
  if df_dominant_topic.Dominant_Topic[i] == 1.0:
    ntopic1.append(df_dominant_topic.Tweet[i])
  if df_dominant_topic.Dominant_Topic[i] == 2.0:
    ntopic2.append(df_dominant_topic.Tweet[i])    
  if df_dominant_topic.Dominant_Topic[i] == 3.0:
    ntopic3.append(df_dominant_topic.Tweet[i])
  if df_dominant_topic.Dominant_Topic[i] == 4.0:
    ntopic4.append(df_dominant_topic.Tweet[i])
  if df_dominant_topic.Dominant_Topic[i] == 5.0:
    ntopic5.append(df_dominant_topic.Tweet[i])
  if df_dominant_topic.Dominant_Topic[i] == 6.0:
    ntopic6.append(df_dominant_topic.Tweet[i])
  if df_dominant_topic.Dominant_Topic[i] == 7.0:
    ntopic7.append(df_dominant_topic.Tweet[i])
  if df_dominant_topic.Dominant_Topic[i] == 8.0:
    ntopic8.append(df_dominant_topic.Tweet[i])
  if df_dominant_topic.Dominant_Topic[i] == 9.0:
    ntopic9.append(df_dominant_topic.Tweet[i])    
  if df_dominant_topic.Dominant_Topic[i] == 10.0:
    ntopic10.append(df_dominant_topic.Tweet[i])
  if df_dominant_topic.Dominant_Topic[i] == 11.0:
    ntopic11.append(df_dominant_topic.Tweet[i])
  if df_dominant_topic.Dominant_Topic[i] == 12.0:
    ntopic12.append(df_dominant_topic.Tweet[i])
  if df_dominant_topic.Dominant_Topic[i] == 13.0:
    ntopic13.append(df_dominant_topic.Tweet[i])

lines_from_dataset = []
for line in train_data['Tweet']:
  lines_from_dataset.append(line)
tokenized_lines = []
for single_line in lines_from_dataset:
  tokenized_lines.append(single_line.split())

import gensim
word_model = gensim.models.Word2Vec(tokenized_lines, min_count=1, sg=1)

# upgrade gensim if you can't import softcossim
from gensim.matutils import softcossim 
from gensim import corpora
import gensim.downloader as api
from gensim.utils import simple_preprocess
print(gensim.__version__)

# Prepare a dictionary and a corpus.
dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in ntopic13])

# Prepare the similarity matrix
similarity_matrix = word_model.wv.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)

similarity_matrix

listToStr = ' '.join([str(elem) for elem in ntopic13])

# Convert the sentences into bag-of-words vectors.
sent_1 = dictionary.doc2bow(simple_preprocess(listToStr))
scs=0
lrg=0
for i in range(len(trigram_HQ['idx'])-1):
  for j in range(len(trigram_HQA['idx'])-1):
    sent_2 = dictionary.doc2bow(simple_preprocess(trigram_HQ['idx'][i]+trigram_HQA['idx'][j]))
    sentences = [sent_1, sent_2]
    scs=softcossim(sent_1, sent_2, similarity_matrix)
    if scs>lrg:
      lrg=scs
      label = trigram_HQ['idx'][i]+trigram_HQA['idx'][j]

lrg

label

# Prepare a dictionary and a corpus.
#dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in ntopic6])

# Prepare the similarity matrix
#similarity_matrix = word_model.wv.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)
#listToStr = ' '.join([str(elem) for elem in ntopic6]) 
#sent_1 = dictionary.doc2bow(simple_preprocess(listToStr))
sent_2 = dictionary.doc2bow(simple_preprocess('(covid)(government)(spread)'))
sentences = [sent_1, sent_2]

# Compute soft cosine similarity
print(softcossim(sent_1, sent_2, similarity_matrix))

#manual
sent_2 = dictionary.doc2bow(simple_preprocess('(government)(response)'))
sentences = [sent_1, sent_2]
print(softcossim(sent_1, sent_2, similarity_matrix))

documents = [listToStr, trigram_HQ['idx'][0]+' '+trigram_HQA['idx'][0]]
# Scikit Learn
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Create the Document Term Matrix
count_vectorizer = CountVectorizer(stop_words='english')
count_vectorizer = CountVectorizer()
sparse_matrix = count_vectorizer.fit_transform(documents)

# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.
doc_term_matrix = sparse_matrix.todense()
df = pd.DataFrame(doc_term_matrix, 
                  columns=count_vectorizer.get_feature_names(),
                  index=['train','test'])

df

from sklearn.metrics.pairwise import cosine_similarity
print(cosine_similarity(df, df))


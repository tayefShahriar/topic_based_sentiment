# -*- coding: utf-8 -*-
"""test_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oCUeXxrc66MJK_WGFTeK_puHWIIbZFsQ
"""

import numpy as np
import pandas as pd
import os
import nltk
import spacy
from wordcloud import WordCloud
import tensorflow as tf
from tensorflow.keras.preprocessing import text
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.utils import to_categorical
from nltk.corpus import stopwords, words
from nltk.stem import WordNetLemmatizer
import gensim #the library for Topic modelling
from gensim.models.ldamulticore import LdaMulticore
from gensim import corpora, models
from itertools import chain
import time
import re
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.metrics import AUC
from sklearn.metrics import confusion_matrix, classification_report
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
stop = set(stopwords.words('english'))

train_data = pd.read_csv('Corona_NLP_train.csv', encoding='latin-1')
test_data = pd.read_csv('Corona_NLP_test.csv', encoding='latin-1')

REPLACE_BY_SPACE = re.compile('[/(){}\[\]\|,;&-_]') 
def preprocess_text(text):
    text = text.lower()                                    
    text = re.sub(r"http\S+", "", text)                    
    text = re.sub(r"\@\S+", "", text)                      
    text = re.sub(r"#\S+", "", text)                       
    text = re.sub(r"won\'t", "would not", text)            
    text = re.sub(r"n\'t", " not", text)                   
    text = REPLACE_BY_SPACE.sub(' ', text)            
    text = [word.strip() for word in text.split()]         
    text = [re.sub(r'[^\u0020-\u007F]+', '', sentence) for sentence in text]
    text = [word for word in text if word not in stop]
    text = [word for word in text if len(word)>2]          
    text = [word for word in text if word!='amp']  
    #text = [word.split() for word in text]        
    text = ' '.join(text)
    return text

train_data['Tweet'] = train_data['OriginalTweet'].apply(preprocess_text)
test_data['Tweet'] = test_data['OriginalTweet'].apply(preprocess_text)

import spacy
nlp = spacy.load('en')

termsA = []
termsS = []
for review in nlp.pipe(train_data.Tweet):
    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']
    termsA.append(' '.join(chunks))
    if review.is_parsed:
        termsS.append(' '.join([token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == "ADJ" or token.pos_ == "VERB"))]))
    else:
        termsS.append('')  
train_data['termsA'] = termsA
train_data['termsS'] = termsS
train_data.head(10)

testA = []
testS = []
for review in nlp.pipe(test_data.Tweet):
    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']
    testA.append(' '.join(chunks))
    if review.is_parsed:
        testS.append(' '.join([token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == "ADJ" or token.pos_ == "VERB"))]))
    else:
        testS.append('')  
test_data['testA'] = testA
test_data['testS'] = testS
test_data.head(10)

train_data['terms'] = train_data['termsA'] + ' ' + train_data['termsS']
test_data['terms'] = test_data['testA'] + ' ' + test_data['testS']

test_data.head()

MAX_NB_WORDS = 20000  #maximum number of words to take from corpus
Tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS, oov_token='<oov>') #initializing tokenizer
Tokenizer.fit_on_texts(train_data['Tweet'].values) #fitting tokenizer on training_datase

word_to_ind = Tokenizer.word_index #extracting word to index mapping from tokenzier

# getting text sequences from training and testing dataframes
X_train = Tokenizer.texts_to_sequences(train_data['terms'].values)
X_test = Tokenizer.texts_to_sequences(test_data['terms'].values)

# calculating maximum length of sequences among both training and testing dataframes
MAXLEN = max([len(x) for x in X_train] + [len(x) for x in X_test])

MAXLEN

X_train_padded = sequence.pad_sequences(X_train, maxlen=MAXLEN)
X_test_padded = sequence.pad_sequences(X_test, maxlen=MAXLEN)

def target(label):
    if label == 'Neutral': 
        return 0
    if label == 'Positive':
        return 2
    if label=='Extremely Positive':
        return 2
    if label=='Negative':
        return 1
    else:
        return 1           
Y_train = train_data['Sentiment'].apply(target).values
Y_test = test_data['Sentiment'].apply(target).values

encoder = LabelEncoder()
Y_train = encoder.fit_transform(Y_train)
Y_test = encoder.transform(Y_test)

#one-hot-encoding sentiment labels
Y_train_enc = to_categorical(Y_train)
Y_test_enc = to_categorical(Y_test)

lines_from_dataset = []
for line in train_data['Tweet']:
  lines_from_dataset.append(line)
tokenized_lines = []
for single_line in lines_from_dataset:
  tokenized_lines.append(single_line.split())

import gensim
word_model = gensim.models.Word2Vec(tokenized_lines, min_count=1, sg=1)

word_model.wv.most_similar(negative=['covid'])

embedding_matrix = np.zeros((len(word_model.wv.vocab)+1, 100))
for i, vec in enumerate(word_model.wv.vectors):
  embedding_matrix[i] = vec

EMBEDDING_DIM = 32

X_trainN = Tokenizer.texts_to_sequences(train_data['Tweet'].values)
X_testN = Tokenizer.texts_to_sequences(test_data['Tweet'].values)

# calculating maximum length of sequences among both training and testing dataframes
MAXLENN = max([len(x) for x in X_trainN] + [len(x) for x in X_testN])

MAXLENN

X_train_paddedN = sequence.pad_sequences(X_trainN, maxlen=MAXLENN)
X_test_paddedN = sequence.pad_sequences(X_testN, maxlen=MAXLENN)

import keras
optimizer = tf.keras.optimizers.RMSprop

from keras.layers.convolutional import Conv1D
from keras.layers import GlobalMaxPooling1D, Flatten, Dropout, GlobalAveragePooling1D, MaxPooling1D, Flatten
model = Sequential(name='Name')
model.add(Embedding(len(word_model.wv.vocab)+1, 100, weights=[embedding_matrix], input_length=MAXLEN))
model.add(GRU(128, return_sequences=True))
model.add(GlobalAveragePooling1D())

tf.keras.backend.clear_session()
import tensorflow.keras.layers as L
units = 256

model2 = tf.keras.Sequential([
    L.Embedding(len(word_model.wv.vocab)+1, 100, weights=[embedding_matrix], input_length=MAXLENN),
    L.Bidirectional(L.LSTM(units, return_sequences=True)),
    L.GlobalAveragePooling1D()
])

#from keras import backend as K
#import keras
#optimizer = keras.optimizers.Adam(lr=0.00001)

len([embedding_matrix])

MAX_NB_WORDS

from keras.models import Model
from keras.layers import concatenate, Activation
from keras.layers import BatchNormalization
from keras.layers.advanced_activations import PReLU
merged_layers = concatenate([model.output, model2.output])
x = Dense(3)(merged_layers)
out = Activation('softmax')(x)
merged_model = Model([model.input, model2.input], [out])
merged_model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy'])

tf.keras.utils.plot_model(merged_model)

from keras.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint("best_model.hdf5", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)

history = merged_model.fit([X_train_padded,X_train_paddedN], Y_train_enc, epochs=10, validation_data=([X_test_padded, X_test_paddedN], Y_test_enc), callbacks=[checkpoint])

plt.rcParams.update({'font.size': 18})
plt.plot(history.history['accuracy'], '-o')
plt.plot(history.history['val_accuracy'], '-o')
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.grid(True)
plt.legend(['train','validation'], loc='best')
plt.figure(figsize=(100,302))
plt.show()

plt.plot(history.history['loss'], '-o')
plt.plot(history.history['val_loss'], '-p')
plt.title('model loss')
plt.ylabel('loss')
plt.grid(True)
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='best')
plt.show()

import keras
best_model = keras.models.load_model("best_model.hdf5")

#evaluating model on test set
Y_pred = best_model.predict([X_test_padded, X_test_paddedN])
Y_pred = np.argmax(Y_pred, axis=1)

from sklearn.metrics import accuracy_score
print(f"The accuracy is : { accuracy_score(Y_test,Y_pred)*100}%")

labels = list(encoder.classes_)

labels = ['Neutral', 'Negative', 'Positive']

#printing classification report
print(classification_report(Y_test, Y_pred, target_names=labels))

cm = confusion_matrix(Y_test, Y_pred)
sns.heatmap(cm, annot=True, xticklabels=labels, yticklabels=labels, fmt='g')
#conf = confusion_matrix(y_test, pred)

